{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/2024/07/flood_prediction_notebook/flood_prediciton_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /teamspace/studios/this_studio/2024/07/flood_prediction_notebook/flood_prediciton_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mlflow\n",
    "import gc\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Models Libs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import svm as svm\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mlflow\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "DROP_COLUMNS = ['id']\n",
    "CV = 5\n",
    "RANDOM_STATE = 42\n",
    "ORIGINAL_COLS = ['id','MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors', 'FloodProbability']\n",
    "\n",
    "global INITIAL_FEATURES\n",
    "INITIAL_FEATURES = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors']\n",
    "\n",
    "# GridSearchCV Params\n",
    "PARAMS_LINEAR_REGRESSION = {}\n",
    "\n",
    "PARAMS_XGB = {}\n",
    "\n",
    "PARAMS_RANDOM_FOREST_REGRESSION = {\n",
    "    'n_estimators':[100,200], \n",
    "    # 'criterion':['squared_error','absolute_error','friedman_mse','poisson']\n",
    "}\n",
    "\n",
    "PARAMS_LASSO = {\n",
    "    'alpha':[0.1,0.01], \n",
    "    'max_iter':[100,500]\n",
    "}\n",
    "\n",
    "PARAMS_RIDGE = {\n",
    "    'alpha':[0.1,0.01], \n",
    "    'max_iter':[100,500]\n",
    "}\n",
    "\n",
    "PARAMS_SVM = {\n",
    "    'C': [1, 10], \n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "PARAMS_DEFAULT = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('../data/train.csv')\n",
    "raw_valid= pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_data,func_num):\n",
    "    fun_dict = {\n",
    "        1: create_features_1,\n",
    "        2: create_features_2,\n",
    "        3: create_features_3,\n",
    "        4: create_features_4,\n",
    "        5: create_features_5,\n",
    "        6: create_features_6,\n",
    "        7: create_features_7\n",
    "    }\n",
    "    feature_func = fun_dict.get(func_num)\n",
    "    df = train_data.copy()\n",
    "    df = feature_func(df)\n",
    "    # Drop Columns\n",
    "    # df.drop(columns=DROP_COLUMNS,axis=1,inplace=True)\n",
    "    return df\n",
    "\n",
    "def evaluate_model(model_name):\n",
    "    # Linear Regression Model\n",
    "    if model_name=='linear_regression':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=LinearRegression(),\n",
    "            param_grid=PARAMS_LINEAR_REGRESSION,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # LASSO\n",
    "    if model_name=='lasso':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=Lasso(),\n",
    "            param_grid=PARAMS_LASSO,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "\n",
    "    # SVM Model\n",
    "    if model_name=='svm':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=svm.SVR(),\n",
    "            param_grid=PARAMS_SVM,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "        print(f'Finished training model {model_name}')\n",
    "\n",
    "    # XGBoost\n",
    "    if model_name=='xgb':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            param_grid=PARAMS_XGB,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "        print(f'Finished training model {model_name}')\n",
    "\n",
    "    # LightGBM\n",
    "    if model_name=='lgbm':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=LGBMRegressor(),\n",
    "            param_grid=PARAMS_DEFAULT,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "        print(f'Finished training model {model_name}')\n",
    "\n",
    "    # CatBoost\n",
    "    if model_name=='catboost':\n",
    "        model =  GridSearchCV(\n",
    "            estimator=CatBoostRegressor(),\n",
    "            param_grid=PARAMS_DEFAULT,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred = model.predict(X_test)\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred)\n",
    "        print(f'Finished training model {model_name}')\n",
    "    \n",
    "    # Ensemble\n",
    "    if model_name=='ensemble':\n",
    "        model_cat =  GridSearchCV(\n",
    "            estimator=CatBoostRegressor(),\n",
    "            param_grid=PARAMS_DEFAULT,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model_cat.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred_cat = model_cat.predict(X_test)\n",
    "\n",
    "        model_xgb =  GridSearchCV(\n",
    "            estimator=xgb.XGBRegressor(),\n",
    "            param_grid=PARAMS_XGB,\n",
    "            cv=CV,\n",
    "            n_jobs=-1,\n",
    "            verbose=2,\n",
    "            scoring='neg_mean_squared_error')\n",
    "        # Train Model\n",
    "        model_xgb.fit(X_train,y_train)\n",
    "        # Predict on Test\n",
    "        y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "        y_pred_final = (y_pred_xgb + y_pred_cat) / 2\n",
    "\n",
    "        model = model_xgb\n",
    "        # Score \n",
    "        score = r2_score(y_test, y_pred_final)\n",
    "        print(f'Finished training model {model_name}')\n",
    "    \n",
    "    # \n",
    "    # Return\n",
    "    return model_name,score,model.best_estimator_,model.best_params_\n",
    "\n",
    "def run_experiment(model_name,exp_desc):\n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "    mlflow.set_experiment(model_name)\n",
    "    with mlflow.start_run():\n",
    "        print('Model Name : ',model_name)\n",
    "        model_name,score,best_model,best_param = evaluate_model(model_name)\n",
    "        mlflow.log_param('drop_columns', DROP_COLUMNS)\n",
    "        mlflow.log_param('model_name',model_name)\n",
    "        mlflow.log_param('desc',exp_desc)\n",
    "        mlflow.log_params(best_param)\n",
    "        mlflow.log_param('cv',CV)\n",
    "        mlflow.log_param('random_state',RANDOM_STATE)\n",
    "        mlflow.log_param('features', str(list(valid_X_scaled.columns)))\n",
    "        mlflow.log_metric('r2',score)\n",
    "        mlflow.sklearn.log_model(best_model,model_name)\n",
    "        # mlflow.log_artifact('transformed_data.csv')\n",
    "    return best_model\n",
    "\n",
    "def get_submission_csv(model):\n",
    "    predictions = model.predict(valid_X_scaled)\n",
    "    sub_df = raw_valid[['id']]\n",
    "    sub_df['FloodProbability'] = predictions\n",
    "    sub_df.to_csv('../data/submission.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_1(data):\n",
    "    df = data.copy()\n",
    "    cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors']\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['sum']= df.sum(axis=1)\n",
    "    df['mean']= df.mean(axis=1)\n",
    "    df['std'] = df.std(axis=1)\n",
    "    df['max'] = df.max(axis=1)\n",
    "    df['min'] = df.min(axis=1)\n",
    "    df['var'] = df.var(axis=1)\n",
    "    df['skew'] = df.skew(axis=1)\n",
    "    df['kurt'] = df.kurt(axis=1)\n",
    "    df['meadian'] = df.median(axis=1)\n",
    "    df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "    df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "    df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "    df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "    df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "    df['range']= df['max']-df['min']\n",
    "    for col in cols:\n",
    "        df[f\"{col}_2\"]= df[col]**2\n",
    "        df[f\"{col}_3\"]= df[col]**3\n",
    "        df[f\"{col}_3\"]= df[col]**4\n",
    "    for col in cols:\n",
    "        if col not in ['id','FloodProbability']:\n",
    "            df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "            df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "            df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "    return df\n",
    "\n",
    "def create_features_2(data):\n",
    "    df = data.copy()\n",
    "    cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors']\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['sum']= df.sum(axis=1)\n",
    "    df['mean']= df.mean(axis=1)\n",
    "    df['std'] = df.std(axis=1)\n",
    "    df['max'] = df.max(axis=1)\n",
    "    df['min'] = df.min(axis=1)\n",
    "    df['var'] = df.var(axis=1)\n",
    "    df['skew'] = df.skew(axis=1)\n",
    "    df['kurt'] = df.kurt(axis=1)\n",
    "    df['meadian'] = df.median(axis=1)\n",
    "    df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "    df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "    df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "    df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "    df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "    df['range']= df['max']-df['min']\n",
    "    for col in cols:\n",
    "        df[f\"{col}_2\"]= df[col]**2\n",
    "        df[f\"{col}_3\"]= df[col]**3\n",
    "        df[f\"{col}_3\"]= df[col]**4\n",
    "        # Log Features\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "    for col in cols:\n",
    "        if col not in ['id','FloodProbability']:\n",
    "            df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "            df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "            df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "    return df\n",
    "\n",
    "def create_features_3(data):\n",
    "    df = data.copy()\n",
    "    cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "       'InadequatePlanning', 'PoliticalFactors']\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "    df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "    df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "    df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "    df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "    df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "    df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "    df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "    df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "    df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "    df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "    df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "    df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "    df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "    df['sum']= df.sum(axis=1) # for tree models \n",
    "    df['product']= df.product(axis=1)\n",
    "    df['special1'] = df['sum'].isin(np.arange(72, 76)) # for linear models\n",
    "    df['special2'] = df['product'].isin(np.arange(72, 76)) \n",
    "    df['mean']= df.mean(axis=1)\n",
    "    df['std'] = df.std(axis=1)\n",
    "    df['max'] = df.max(axis=1)\n",
    "    df['min'] = df.min(axis=1)\n",
    "    df['var'] = df.var(axis=1)\n",
    "    df['skew'] = df.skew(axis=1)\n",
    "    df['kurt'] = df.kurt(axis=1)\n",
    "    df['meadian'] = df.median(axis=1)\n",
    "    df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "    df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "    df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "    df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "    df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "    df['range']= df['max']-df['min']\n",
    "    for col in cols:\n",
    "        df[f\"{col}_2\"]= df[col]**2\n",
    "        df[f\"{col}_3\"]= df[col]**3\n",
    "        df[f\"{col}_3\"]= df[col]**4\n",
    "        # Log Features\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)\n",
    "        df[f\"log_{col}\"] = np.log2(df[col]+1e-4) \n",
    "\n",
    "    for col in cols:\n",
    "        if col not in ['id','FloodProbability']:\n",
    "            df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "            df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "            df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "    return df\n",
    "\n",
    "def create_features_4(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "    df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "\n",
    "    log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "    log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "    exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "    new_cols = []\n",
    "\n",
    "    df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "    df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "    df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "    df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "    df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "    df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "    df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "    df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "    df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "    df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "    df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "    feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']\n",
    "    df = df[feats]\n",
    "    return df \n",
    "\n",
    "def create_features_5(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "    df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "    df['special1'] = np.where(df['special1']==True,1,0)\n",
    "\n",
    "    log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "    log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "    exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "    new_cols = []\n",
    "\n",
    "    df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "    df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "    df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "    df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "    df['special2'] = np.where(df['special2']==True,1,0)\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "    df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "    df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "    df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "    df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "    df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "    df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "    df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "    feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']+['special1','special2']\n",
    "    df = df[feats]\n",
    "    return df \n",
    "\n",
    "def create_features_6(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "    df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "    df['special1'] = np.where(df['special1']==True,1,0)\n",
    "\n",
    "    log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "    log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "    exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "    new_cols = []\n",
    "\n",
    "    df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "    df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "    df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "    df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "    df['special2'] = np.where(df['special2']==True,1,0)\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "    df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "    df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "    df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "    df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "    df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "    df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "    df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "    feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']+log_features+['special1','special2']\n",
    "    df = df[feats]\n",
    "    return df \n",
    "\n",
    "def create_features_7(data):\n",
    "    df = data.copy()\n",
    "\n",
    "    df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "    df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "    df['special1'] = np.where(df['special1']==True,1,0)\n",
    "\n",
    "    log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "    log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "    exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "    exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "    new_cols = []\n",
    "\n",
    "    df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "    df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "    df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "    df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "    df['special2'] = np.where(df['special2']==True,1,0)\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "    df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "    df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "    df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "    df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "    df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "    df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "    for col in INITIAL_FEATURES:\n",
    "        df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "    df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "    feats = list(INITIAL_FEATURES)+['fsum','fsum2','exp_sum','log_sum','exp2_sum','exp3_sum']\n",
    "    df = df[feats]\n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split and Scaling\n",
    "raw_train_X = raw_train.drop('FloodProbability',axis=1)\n",
    "raw_train_y = raw_train['FloodProbability']\n",
    "\n",
    "feature_func = 7\n",
    "\n",
    "train_X = process_data(raw_train_X,feature_func)\n",
    "train_y = raw_train_y\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "train_X_scaled = sc.fit_transform(train_X)\n",
    "train_X_scaled = pd.DataFrame(train_X_scaled,columns=train_X.columns)\n",
    "# train_X_scaled = train_X.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_scaled, train_y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "valid_X = process_data(raw_valid,feature_func)\n",
    "valid_X_scaled = sc.transform(valid_X)\n",
    "valid_X_scaled = pd.DataFrame(valid_X_scaled,columns=valid_X.columns)\n",
    "# valid_X_scaled = valid_X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del raw_train_X,raw_train_y,train_X,train_y,train_X_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Name :  xgb\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV] END .................................................... total time=  15.3s\n",
      "[CV] END .................................................... total time=  15.3s\n",
      "[CV] END .................................................... total time=  15.4s\n",
      "[CV] END .................................................... total time=  15.4s\n",
      "[CV] END .................................................... total time=   8.4s\n",
      "Finished training model xgb\n"
     ]
    }
   ],
   "source": [
    "Experiment_Desc = 'Feature Func : 7 , CV = 6'\n",
    "# md_lr = run_experiment('linear_regression',Experiment_Desc)\n",
    "# md_lgbm =run_experiment('lgbm',Experiment_Desc)\n",
    "md_xgb = run_experiment('xgb',Experiment_Desc)\n",
    "# md_catboost =run_experiment('catboost',Experiment_Desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_submission_csv(md_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
