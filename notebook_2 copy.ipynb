{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/2024/07/flood_prediction_notebook/flood_prediciton_code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /teamspace/studios/this_studio/2024/07/flood_prediction_notebook/flood_prediciton_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import mlflow\n",
    "import gc\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Models Libs\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import svm as svm\n",
    "import lightgbm as lgbm\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Metric\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Mlflow\n",
    "import mlflow\n",
    "\n",
    "# Features\n",
    "import imp\n",
    "import features as features\n",
    "import config as config\n",
    "imp.reload(features)\n",
    "imp.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('../data/train.csv')\n",
    "raw_valid= pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_features_1(data):\n",
    "#     df = data.copy()\n",
    "#     cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "#        'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "#        'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "#        'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "#        'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "#        'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "#        'InadequatePlanning', 'PoliticalFactors']\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['sum']= df.sum(axis=1)\n",
    "#     df['mean']= df.mean(axis=1)\n",
    "#     df['std'] = df.std(axis=1)\n",
    "#     df['max'] = df.max(axis=1)\n",
    "#     df['min'] = df.min(axis=1)\n",
    "#     df['var'] = df.var(axis=1)\n",
    "#     df['skew'] = df.skew(axis=1)\n",
    "#     df['kurt'] = df.kurt(axis=1)\n",
    "#     df['meadian'] = df.median(axis=1)\n",
    "#     df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "#     df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "#     df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "#     df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "#     df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "#     df['range']= df['max']-df['min']\n",
    "#     for col in cols:\n",
    "#         df[f\"{col}_2\"]= df[col]**2\n",
    "#         df[f\"{col}_3\"]= df[col]**3\n",
    "#         df[f\"{col}_3\"]= df[col]**4\n",
    "#     for col in cols:\n",
    "#         if col not in ['id','FloodProbability']:\n",
    "#             df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "#             df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "#             df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "#     return df\n",
    "\n",
    "# def create_features_2(data):\n",
    "#     df = data.copy()\n",
    "#     cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "#        'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "#        'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "#        'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "#        'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "#        'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "#        'InadequatePlanning', 'PoliticalFactors']\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['sum']= df.sum(axis=1)\n",
    "#     df['mean']= df.mean(axis=1)\n",
    "#     df['std'] = df.std(axis=1)\n",
    "#     df['max'] = df.max(axis=1)\n",
    "#     df['min'] = df.min(axis=1)\n",
    "#     df['var'] = df.var(axis=1)\n",
    "#     df['skew'] = df.skew(axis=1)\n",
    "#     df['kurt'] = df.kurt(axis=1)\n",
    "#     df['meadian'] = df.median(axis=1)\n",
    "#     df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "#     df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "#     df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "#     df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "#     df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "#     df['range']= df['max']-df['min']\n",
    "#     for col in cols:\n",
    "#         df[f\"{col}_2\"]= df[col]**2\n",
    "#         df[f\"{col}_3\"]= df[col]**3\n",
    "#         df[f\"{col}_3\"]= df[col]**4\n",
    "#         # Log Features\n",
    "#         df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "#     for col in cols:\n",
    "#         if col not in ['id','FloodProbability']:\n",
    "#             df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "#             df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "#             df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "#     return df\n",
    "\n",
    "# def create_features_3(data):\n",
    "#     df = data.copy()\n",
    "#     cols = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
    "#        'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
    "#        'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
    "#        'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
    "#        'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
    "#        'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
    "#        'InadequatePlanning', 'PoliticalFactors']\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure'] = df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['ClimateImpact'] = df.MonsoonIntensity+df.ClimateChange\n",
    "#     df['AnthropogenicPressure'] = df.Deforestation+df.Urbanization+df.AgriculturalPractices+df.Encroachments\n",
    "#     df['InfrastructureQuality'] = df.DamsQuality+df.DrainageSystems+df.DeterioratingInfrastructure\n",
    "#     df['CoastalVulnerabilityTotal'] = df.CoastalVulnerability+df.Landslides\n",
    "#     df['PreventiveMeasuresEfficiency'] = df.RiverManagement+df.IneffectiveDisasterPreparedness+df.InadequatePlanning\n",
    "#     df['EcosystemImpact'] = df.WetlandLoss+df.Watersheds\n",
    "#     df['SocioPoliticalContext'] = df.PopulationScore+df.PoliticalFactors\n",
    "#     df['Land_Use_Pressure']=df.Urbanization+df.Deforestation+df.AgriculturalPractices\n",
    "#     df['Environmental_Degradation']=df.Deforestation+df.Siltation+df.WetlandLoss+df.Landslides\n",
    "#     df['Infrastructure_Vulnerability'] = df.DeterioratingInfrastructure+df.InadequatePlanning\n",
    "#     df['Community_Preparednessg']= df.IneffectiveDisasterPreparedness+df.PoliticalFactors\n",
    "#     df['Population_Density_Vulnerable_Areas']= df.PopulationScore+df.CoastalVulnerability\n",
    "#     df['Climate_Change_Impact']= df.ClimateChange+df.MonsoonIntensity\n",
    "#     df['River_Health']= df.RiverManagement+df.DamsQuality\n",
    "#     df['sum']= df.sum(axis=1) # for tree models \n",
    "#     df['product']= df.product(axis=1)\n",
    "#     df['special1'] = df['sum'].isin(np.arange(72, 76)) # for linear models\n",
    "#     df['special2'] = df['product'].isin(np.arange(72, 76)) \n",
    "#     df['mean']= df.mean(axis=1)\n",
    "#     df['std'] = df.std(axis=1)\n",
    "#     df['max'] = df.max(axis=1)\n",
    "#     df['min'] = df.min(axis=1)\n",
    "#     df['var'] = df.var(axis=1)\n",
    "#     df['skew'] = df.skew(axis=1)\n",
    "#     df['kurt'] = df.kurt(axis=1)\n",
    "#     df['meadian'] = df.median(axis=1)\n",
    "#     df['quant_25'] = df.quantile(0.25,axis=1)\n",
    "#     df['quant_75'] = df.quantile(0.75,axis=1)\n",
    "#     df['sum>72'] = np.where(df['sum']>72,1,0)\n",
    "#     df['sum>100'] = np.where(df['sum']>100,1,0)\n",
    "#     df['sum>50'] = np.where(df['sum']>50,1,0)\n",
    "#     df['range']= df['max']-df['min']\n",
    "#     for col in cols:\n",
    "#         df[f\"{col}_2\"]= df[col]**2\n",
    "#         df[f\"{col}_3\"]= df[col]**3\n",
    "#         df[f\"{col}_3\"]= df[col]**4\n",
    "#         # Log Features\n",
    "#         df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)\n",
    "#         df[f\"log_{col}\"] = np.log2(df[col]+1e-4) \n",
    "\n",
    "#     for col in cols:\n",
    "#         if col not in ['id','FloodProbability']:\n",
    "#             df[f\"mad_{col}\"] = df[col] - df[col].median()\n",
    "#             df[f\"mean_{col}\"] = df[col] - df[col].mean()\n",
    "#             df[f\"std_{col}\"] = df[col] - df[col].std()\n",
    "#     return df\n",
    "\n",
    "# def create_features_4(data):\n",
    "#     df = data.copy()\n",
    "\n",
    "#     df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "#     df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "\n",
    "#     log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "#     log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "#     exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "#     new_cols = []\n",
    "\n",
    "#     df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "#     df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "#     df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "#     df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "#     df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "#     df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "#     df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "#     df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "#     df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "#     df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "#     df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "#     feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']\n",
    "#     df = df[feats]\n",
    "#     return df \n",
    "\n",
    "# def create_features_5(data):\n",
    "#     df = data.copy()\n",
    "\n",
    "#     df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "#     df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "#     df['special1'] = np.where(df['special1']==True,1,0)\n",
    "\n",
    "#     log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "#     log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "#     exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "#     new_cols = []\n",
    "\n",
    "#     df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "#     df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "#     df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "#     df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "#     df['special2'] = np.where(df['special2']==True,1,0)\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "#     df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "#     df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "#     df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "#     df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "#     df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "#     df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "#     df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "#     feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']+['special1','special2']\n",
    "#     df = df[feats]\n",
    "#     return df \n",
    "\n",
    "# def create_features_6(data):\n",
    "#     df = data.copy()\n",
    "\n",
    "#     df['fsum'] = df[INITIAL_FEATURES].sum(axis=1) # for tree models\n",
    "#     df['special1'] = df['fsum'].isin(np.arange(72, 76)) # for linear models\n",
    "#     df['special1'] = np.where(df['special1']==True,1,0)\n",
    "\n",
    "#     log_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "#     log2_features = [f\"log_{col}\" for col in INITIAL_FEATURES]\n",
    "\n",
    "#     exp_features = [f\"exp_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp2_features = [f\"exp2_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp3_features = [f\"exp3_{col}\" for col in INITIAL_FEATURES]\n",
    "#     exp4_features = [f\"exp4_{col}\" for col in INITIAL_FEATURES]\n",
    "#     new_cols = []\n",
    "\n",
    "#     df['fsum2'] = df[INITIAL_FEATURES].product(axis=1)\n",
    "#     df['zero_count'] = (df[INITIAL_FEATURES] < 10).sum(axis=1)\n",
    "#     df['one_count'] = (df[INITIAL_FEATURES] > 10).sum(axis=1)\n",
    "    \n",
    "#     df['special2'] = df['fsum2'].isin(np.arange(72, 76)) \n",
    "#     df['special2'] = np.where(df['special2']==True,1,0)\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log_{col}\"] = np.log1p(df[col]+1e-4)  \n",
    "#     df['log_sum'] = df[log_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"log2_{col}\"] = np.log2(df[col]+1e-4)  \n",
    "#     df['log2_sum'] = df[log2_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp_{col}\"] = 1.2**(df[col])\n",
    "\n",
    "#     df['exp_sum'] = df[exp_features].sum(axis=1)\n",
    "#     df['exp_prod'] = df[exp_features].product(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp2_{col}\"] = np.exp(df[col])\n",
    "#     df['exp2_sum'] = df[exp2_features].sum(axis=1)\n",
    "\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp3_{col}\"] = 4**(df[col])\n",
    "#     df['exp3_sum'] = df[exp3_features].sum(axis=1)\n",
    "\n",
    "#     for col in INITIAL_FEATURES:\n",
    "#         df[f\"exp4_{col}\"] = 6**(df[col])\n",
    "#     df['exp4_sum'] = df[exp4_features].sum(axis=1)\n",
    "\n",
    "#     feats = list(INITIAL_FEATURES)+['fsum','one_count','fsum2','exp_sum','log_sum','log2_sum','exp2_sum','exp3_sum']+log_features+['special1','special2']\n",
    "#     df = df[feats]\n",
    "#     return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m raw_train_y \u001b[38;5;241m=\u001b[39m raw_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFloodProbability\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m feature_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m----> 7\u001b[0m train_X \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_train_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeature_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m train_y \u001b[38;5;241m=\u001b[39m raw_train_y\n\u001b[1;32m     10\u001b[0m sc \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36mprocess_data\u001b[0;34m(train_data, func_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m feature_func \u001b[38;5;241m=\u001b[39m fun_dict\u001b[38;5;241m.\u001b[39mget(func_num)\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mINITIAL_FEATURES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Drop Columns\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# df.drop(columns=DROP_COLUMNS,axis=1,inplace=True)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/2024/07/flood_prediction_notebook/flood_prediciton_code/features.py:256\u001b[0m, in \u001b[0;36mcreate_features_5\u001b[0;34m(data, initial_features)\u001b[0m\n\u001b[1;32m    253\u001b[0m df \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    255\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfsum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[initial_features]\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# for tree models\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfsum\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m72\u001b[39m, \u001b[38;5;241m76\u001b[39m)) \u001b[38;5;66;03m# for linear models\u001b[39;00m\n\u001b[1;32m    257\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecial1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    259\u001b[0m log_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m initial_features]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Train Test Split and Scaling\n",
    "raw_train_X = raw_train.drop('FloodProbability',axis=1)\n",
    "raw_train_y = raw_train['FloodProbability']\n",
    "\n",
    "feature_func = 5\n",
    "\n",
    "train_X = process_data(raw_train_X,feature_func)\n",
    "train_y = raw_train_y\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "sc.fit(train_X)\n",
    "train_X_scaled = sc.transform(train_X)\n",
    "train_X_scaled = pd.DataFrame(train_X,columns=train_X.columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X_scaled, train_y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "valid_X = process_data(raw_valid,feature_func)\n",
    "valid_X_scaled = sc.transform(valid_X)\n",
    "valid_X_scaled = pd.DataFrame(valid_X_scaled,columns=valid_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del raw_train_X,raw_train_y,train_X,train_y,train_X_scaled\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiment_Desc = 'Feature Func : 6 , CV = 6'\n",
    "# md_lr = run_experiment('linear_regression',Experiment_Desc)\n",
    "# md_lgbm =run_experiment('lgbm',Experiment_Desc)\n",
    "md_xgb = run_experiment('xgb',Experiment_Desc)\n",
    "# md_catboost =run_experiment('catboost',Experiment_Desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
